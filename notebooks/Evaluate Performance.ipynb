{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "43a704cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from collections import defaultdict\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import scipy.stats as st\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colorbar as colorbar\n",
    "from statsmodels.stats.contingency_tables import cochrans_q, mcnemar, Table\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "43329b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESAP questions with images\n",
    "q_w_images = [3, 4, 6, 32, 35, 54, 68, 88]\n",
    "\n",
    "cases = np.delete(np.arange(100), [0] + q_w_images) # valid questions\n",
    "n_cases = cases.size\n",
    "\n",
    "# Available options\n",
    "options = ['A', 'B', 'C', 'D', 'E']\n",
    "# even for the experiments without letter, manual annotator saved the option using letter.\n",
    "\n",
    "path_to_results = \"./../results\"\n",
    "\n",
    "model_file_name_dict = {'huatuo-o1': 'HuatuoGPT-o1',\n",
    "                        'diabetica-o1': 'Diabetica-o1',\n",
    "                        'diabetica-7B': 'Diabetica-7B',\n",
    "                        'meditron3-8B': 'Meditron3-8B'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b5e0ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response_distrib(filepath):\n",
    "    \"\"\" \n",
    "    Given the excel file, this function parses and standardize the file entries.\n",
    "    Returns a counter in the form of a dictionary.\n",
    "    \"\"\"\n",
    "    df_ = pd.read_excel(filepath, header=None, index_col=0)\n",
    "    \n",
    "    # everything capital\n",
    "    df_ = df_.map(lambda x: x.upper() if isinstance(x, str) else x) \n",
    "    \n",
    "    # remove NaN values - this remove extra-columns\n",
    "    df_ = df_.dropna(axis=1, how='all') \n",
    "    if not np.array_equal(df_.index, cases): # we keep all 91 cases\n",
    "        raise ValueError(\"We are removing a valid case by mistake! Check Excel file\")\n",
    "\n",
    "    # unique annotations\n",
    "    unique_elements = np.unique(df_.values) \n",
    "\n",
    "    # multiple options selected\n",
    "    # are indicated as option1-option2: there is a dash\n",
    "    bm_multiple = [\"-\" in v_ for v_ in unique_elements] \n",
    "    \n",
    "    bm_hall_none = [True if (v_ == \"HALL\" or v_ == 'NONE') # hallucination or no response\n",
    "                    else False for v_ in unique_elements]\n",
    "    \n",
    "    bm_others = ~np.logical_or(bm_multiple, bm_hall_none) # correct answers / check (temporary)\n",
    "    \n",
    "    idx_multiple = np.where(bm_multiple)[0] # indexes\n",
    "    idx_hall_none = np.where(bm_hall_none)[0]\n",
    "        \n",
    "    dct_counter = {}\n",
    "    all_ = 0 # to make sure we did not miss any response (91 or multiple for 10 rep)\n",
    "\n",
    "    # we are creating a new label for multi and hall/none\n",
    "    if len(idx_multiple) > 0:\n",
    "        count = 0\n",
    "        for v_ in unique_elements[idx_multiple]:\n",
    "            count += (df_.values == v_).sum()\n",
    "        all_ = count\n",
    "        dct_counter[\"MULTI\"] = count\n",
    "            \n",
    "    if len(idx_hall_none) > 0:\n",
    "        count = 0\n",
    "        for v_ in unique_elements[idx_hall_none]:\n",
    "            count += (df_.values == v_).sum()\n",
    "        all_ += count\n",
    "        dct_counter[\"HALL / NONE\"] = count\n",
    "       \n",
    "    # we are using the old label for everything else\n",
    "    for element in unique_elements[bm_others]:\n",
    "        count = (df_.values == element).sum()\n",
    "        dct_counter[element] = count\n",
    "        all_ += count\n",
    "            \n",
    "    list_add = [\"CHECK\", \"MULTI\", \"HALL / NONE\", \"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "    for el_ in list_add:\n",
    "        if el_ not in dct_counter.keys():\n",
    "            dct_counter[el_] = 0\n",
    "    dct_counter[\"TOTAL\"] = all_\n",
    "        \n",
    "    return dct_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d1c38",
   "metadata": {},
   "source": [
    "# Experiment 1: Check Performance & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b2ee8",
   "metadata": {},
   "source": [
    "Filenames for LLMs' outputs under prompt 1 and prompt 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8f341a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths_token = {'meditron3-8B-prompt1': 'meditron3-8B_promptID_001_output_4.xlsx',\n",
    "                     'huatuo-o1-prompt1': 'huatuo-o1_promptID_001_output_9.xlsx',\n",
    "                     'huatuo-o1-prompt2': 'huatuo-o1_promptID_002_output_11.xlsx',\n",
    "                     'diabetica-o1-prompt1': 'diabetica-o1_promptID_001_output_2.xlsx',\n",
    "                     'diabetica-o1-prompt2': 'diabetica-o1_promptID_002_output_3.xlsx',\n",
    "                     'diabetica-7B-prompt1': 'diabetica-7B_promptID_001_output_3.xlsx',\n",
    "                     'diabetica-7B-prompt2': 'diabetica-7B_promptID_002_output_4.xlsx',\n",
    "                     'medfound7B-prompt1': 'medfound7B_promptID_001_output_2.xlsx',\n",
    "                     'medfound7B-prompt2': 'medfound7B_promptID_002_output_3.xlsx',\n",
    "                     'clinical-chatgpt-prompt1': 'clinical-chatgpt_promptID_001_output_2.xlsx',\n",
    "                     'clinical-chatgpt-prompt2': 'clinical-chatgpt_promptID_002_output_3.xlsx'\n",
    "                    }\n",
    "\n",
    "model_paths_no_token = {'huatuo-o1': 'huatuo-o1_promptID_001_output_47.xlsx',\n",
    "                        'diabetica-o1': 'diabetica-o1_promptID_001_output_40.xlsx',\n",
    "                        'diabetica-7B': 'diabetica-7B_promptID_001_output_47.xlsx',\n",
    "                        'meditron3-8B': 'meditron3-8B_promptID_001_output_39.xlsx',\n",
    "                        'clinical-chatgpt': 'clinical-chatgpt_promptID_001_output_4.xlsx',\n",
    "                        'medfound7B': 'medfound7B_promptID_001_output_4.xlsx'\n",
    "                       }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151eea75",
   "metadata": {},
   "source": [
    "### **Function to read excel and count entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f09e5cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_extensive_dfs(result_folder, dct_models, letter=True):\n",
    "    \"\"\" \n",
    "    Used in experiment 1 to a) keep track of distribution of responses\n",
    "    b) build dataframe with extensive results.\n",
    "    \n",
    "    Variables: \n",
    "    result_folder: str, folder containing model's output\n",
    "    dct_models: str, excel file \n",
    "    letter: bool, if True, ESAP question, otherwise, letter removed from option\n",
    "    \n",
    "    Returns:\n",
    "    df_counter: pd.DataFrame, distribution of models responses\n",
    "    df_all: pd.DataFrame extensive (91 outputs)\n",
    "    \"\"\"\n",
    "    list_df = []\n",
    "    index = []\n",
    "    counting_outputs = []\n",
    "\n",
    "    for model_, file_ in dct_models.items():\n",
    "        if letter:\n",
    "            filepath = join(path_to_results, model_.split(\"-prompt\")[0], file_)\n",
    "        else: \n",
    "            filepath = join(path_to_results, \"NO_LETTERS\", file_)\n",
    "            \n",
    "        df_ = pd.read_excel(filepath, header=None, index_col=0)\n",
    "        df_ = df_[1] # we keep first column, others are empty\n",
    "        list_df.append(df_)\n",
    "        \n",
    "        dct_counter = evaluate_response_distrib(filepath)\n",
    "        counting_outputs.append([dct_counter[k] for k in sorted(dct_counter.keys())])\n",
    "\n",
    "    # this is for the counter\n",
    "    columns_name = sorted(dct_counter.keys())\n",
    "    list_model_name = [m_ if letter else f\"{m_}_no_letter\" for m_ in dct_models.keys()]\n",
    "    df_counter = pd.DataFrame(data=counting_outputs, \n",
    "                              index=list_model_name,\n",
    "                              columns=columns_name)\n",
    "\n",
    "    # this is for the Df with 91 responses\n",
    "    df_all = pd.concat(list_df, axis=1)\n",
    "    df_all.columns = list_model_name\n",
    "    \n",
    "    return df_counter, df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a63fca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counter_prompt_A_B, df_all = counter_extensive_dfs(path_to_results, model_paths_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b8d90d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counter_no_let, df_all_no_let = counter_extensive_dfs(path_to_results, model_paths_no_token, letter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "acd7873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"tables\", exist_ok=True)\n",
    "pd.concat([df_counter_prompt_A_B, df_counter_no_let]).to_latex(\"tables/readings_experiment_1.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ed2f44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all results from experiment 1\n",
    "df_all = pd.concat([df_all, df_all_no_let], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a1a4e",
   "metadata": {},
   "source": [
    "Loading ground-truth responses from ESAP, after being manually converted into json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e36441f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./../inference/Ped-ESAP.json\", \"r\") as f:\n",
    "    content = json.load(f)\n",
    "    \n",
    "responses = []\n",
    "labs = []\n",
    "table = []\n",
    "for k, v in content.items():\n",
    "    if len(v['answer']) > 0:\n",
    "        responses.append(v['answer'][0])\n",
    "        labs.append(v[\"labs\"]=='Yes')\n",
    "        table.append(v[\"table\"]=='Yes')\n",
    "    else:\n",
    "        responses.append(None)\n",
    "        labs.append(None)\n",
    "        table.append(None)\n",
    "        \n",
    "ground_truth = pd.DataFrame(data=np.array(responses[:-1]).reshape(-1,1), index=np.arange(1,100),\n",
    "                           columns=[\"truth\"])\n",
    "ground_truth = ground_truth.drop(q_w_images)\n",
    "\n",
    "meta_labs_table = np.hstack((np.array(labs).reshape(-1,1), np.array(table).reshape(-1,1)))\n",
    "meta_labs_tab_df = pd.DataFrame(data=meta_labs_table[:-1, :], index=np.arange(1,100),\n",
    "                    columns=[\"labs\", \"table\"])\n",
    "meta_labs_tab_df = meta_labs_tab_df.drop(q_w_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6a7b9f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labs</th>\n",
       "      <th>table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     labs table\n",
       "9    True  True\n",
       "24   True  True\n",
       "36   True  True\n",
       "73  False  True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_labs_tab_df[meta_labs_tab_df[\"table\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216698d",
   "metadata": {},
   "source": [
    "Ground truth is concatenated to table as last column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "02008839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run only once, otherwise we get multiple \"truth\" columns\n",
    "df_all = pd.concat([df_all, ground_truth], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8519f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we keep the value in entry if is one among A, B, C, D, or E\n",
    "# we report as NaN otherwise\n",
    "for col in df_all.columns[:-1]:\n",
    "    df_all[col] = df_all[col].where(df_all[col].isin(options), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bba6553a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meditron3-8B-prompt1</th>\n",
       "      <th>huatuo-o1-prompt1</th>\n",
       "      <th>huatuo-o1-prompt2</th>\n",
       "      <th>diabetica-o1-prompt1</th>\n",
       "      <th>diabetica-o1-prompt2</th>\n",
       "      <th>diabetica-7B-prompt1</th>\n",
       "      <th>diabetica-7B-prompt2</th>\n",
       "      <th>medfound7B-prompt1</th>\n",
       "      <th>medfound7B-prompt2</th>\n",
       "      <th>clinical-chatgpt-prompt1</th>\n",
       "      <th>clinical-chatgpt-prompt2</th>\n",
       "      <th>huatuo-o1_no_letter</th>\n",
       "      <th>diabetica-o1_no_letter</th>\n",
       "      <th>diabetica-7B_no_letter</th>\n",
       "      <th>meditron3-8B_no_letter</th>\n",
       "      <th>clinical-chatgpt_no_letter</th>\n",
       "      <th>medfound7B_no_letter</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  meditron3-8B-prompt1 huatuo-o1-prompt1 huatuo-o1-prompt2  \\\n",
       "1                    B                 B                 B   \n",
       "2                    A                 C                 A   \n",
       "5                    A                 C                 A   \n",
       "7                    C                 E                 E   \n",
       "8                    A                 A                 A   \n",
       "\n",
       "  diabetica-o1-prompt1 diabetica-o1-prompt2 diabetica-7B-prompt1  \\\n",
       "1                    B                    B                  NaN   \n",
       "2                    A                    E                    D   \n",
       "5                    B                    C                    C   \n",
       "7                    C                    C                    D   \n",
       "8                    A                    C                    C   \n",
       "\n",
       "  diabetica-7B-prompt2 medfound7B-prompt1 medfound7B-prompt2  \\\n",
       "1                    D                NaN                NaN   \n",
       "2                    D                NaN                NaN   \n",
       "5                    B                NaN                NaN   \n",
       "7                    A                NaN                NaN   \n",
       "8                    C                NaN                NaN   \n",
       "\n",
       "  clinical-chatgpt-prompt1 clinical-chatgpt-prompt2 huatuo-o1_no_letter  \\\n",
       "1                        B                        C                   B   \n",
       "2                        D                        D                   C   \n",
       "5                        A                        A                   B   \n",
       "7                        D                        A                   A   \n",
       "8                        A                        A                   A   \n",
       "\n",
       "  diabetica-o1_no_letter diabetica-7B_no_letter meditron3-8B_no_letter  \\\n",
       "1                      C                      D                      B   \n",
       "2                      A                      D                    NaN   \n",
       "5                      B                      B                      B   \n",
       "7                      E                      A                      E   \n",
       "8                      B                    NaN                      A   \n",
       "\n",
       "  clinical-chatgpt_no_letter medfound7B_no_letter truth  \n",
       "1                          D                  NaN     B  \n",
       "2                          D                  NaN     A  \n",
       "5                          A                  NaN     C  \n",
       "7                          D                  NaN     A  \n",
       "8                          A                  NaN     E  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "26991631",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_valid = ~df_all.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d645b186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meditron3-8B-prompt1</th>\n",
       "      <th>huatuo-o1-prompt1</th>\n",
       "      <th>huatuo-o1-prompt2</th>\n",
       "      <th>diabetica-o1-prompt1</th>\n",
       "      <th>diabetica-o1-prompt2</th>\n",
       "      <th>diabetica-7B-prompt1</th>\n",
       "      <th>diabetica-7B-prompt2</th>\n",
       "      <th>medfound7B-prompt1</th>\n",
       "      <th>medfound7B-prompt2</th>\n",
       "      <th>clinical-chatgpt-prompt1</th>\n",
       "      <th>clinical-chatgpt-prompt2</th>\n",
       "      <th>huatuo-o1_no_letter</th>\n",
       "      <th>diabetica-o1_no_letter</th>\n",
       "      <th>diabetica-7B_no_letter</th>\n",
       "      <th>meditron3-8B_no_letter</th>\n",
       "      <th>clinical-chatgpt_no_letter</th>\n",
       "      <th>medfound7B_no_letter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   meditron3-8B-prompt1  huatuo-o1-prompt1  huatuo-o1-prompt2  \\\n",
       "1                  True               True               True   \n",
       "2                  True              False               True   \n",
       "5                 False               True              False   \n",
       "7                 False              False              False   \n",
       "8                 False              False              False   \n",
       "\n",
       "   diabetica-o1-prompt1  diabetica-o1-prompt2  diabetica-7B-prompt1  \\\n",
       "1                  True                  True                 False   \n",
       "2                  True                 False                 False   \n",
       "5                 False                  True                  True   \n",
       "7                 False                 False                 False   \n",
       "8                 False                 False                 False   \n",
       "\n",
       "   diabetica-7B-prompt2  medfound7B-prompt1  medfound7B-prompt2  \\\n",
       "1                 False               False               False   \n",
       "2                 False               False               False   \n",
       "5                 False               False               False   \n",
       "7                  True               False               False   \n",
       "8                 False               False               False   \n",
       "\n",
       "   clinical-chatgpt-prompt1  clinical-chatgpt-prompt2  huatuo-o1_no_letter  \\\n",
       "1                      True                     False                 True   \n",
       "2                     False                     False                False   \n",
       "5                     False                     False                False   \n",
       "7                     False                      True                 True   \n",
       "8                     False                     False                False   \n",
       "\n",
       "   diabetica-o1_no_letter  diabetica-7B_no_letter  meditron3-8B_no_letter  \\\n",
       "1                   False                   False                    True   \n",
       "2                    True                   False                   False   \n",
       "5                   False                   False                   False   \n",
       "7                   False                    True                   False   \n",
       "8                   False                   False                   False   \n",
       "\n",
       "   clinical-chatgpt_no_letter  medfound7B_no_letter  \n",
       "1                       False                 False  \n",
       "2                       False                 False  \n",
       "5                       False                 False  \n",
       "7                       False                 False  \n",
       "8                       False                 False  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df = df_all[df_all.columns[:-1]].eq(df_all['truth'], axis=0)\n",
    "\n",
    "accuracy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c3dc8b",
   "metadata": {},
   "source": [
    "We sum each column to obtain the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d2a9aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = accuracy_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9c3b7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_col = df_all.columns[:-1] # we exclude the truth column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715ca08",
   "metadata": {},
   "source": [
    "We call interpretable the case where a model is able to give a unique answer, which is one of the options specified by ESAP.\n",
    "\n",
    "#### Double checking - the number in the following table corresponds with the more extensive ones in the article (Appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c2ed62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([accuracy_scores[models_col], bm_valid[models_col].sum()], axis=1)\n",
    "results.columns = ['# correct', '# interpretable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a70d0c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># correct</th>\n",
       "      <th># interpretable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>meditron3-8B-prompt1</th>\n",
       "      <td>0.329670</td>\n",
       "      <td>0.978022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huatuo-o1-prompt1</th>\n",
       "      <td>0.351648</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huatuo-o1-prompt2</th>\n",
       "      <td>0.351648</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-o1-prompt1</th>\n",
       "      <td>0.329670</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-o1-prompt2</th>\n",
       "      <td>0.340659</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-7B-prompt1</th>\n",
       "      <td>0.296703</td>\n",
       "      <td>0.989011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-7B-prompt2</th>\n",
       "      <td>0.318681</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medfound7B-prompt1</th>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.186813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medfound7B-prompt2</th>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical-chatgpt-prompt1</th>\n",
       "      <td>0.197802</td>\n",
       "      <td>0.791209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical-chatgpt-prompt2</th>\n",
       "      <td>0.197802</td>\n",
       "      <td>0.945055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huatuo-o1_no_letter</th>\n",
       "      <td>0.329670</td>\n",
       "      <td>0.967033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-o1_no_letter</th>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.945055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-7B_no_letter</th>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.879121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meditron3-8B_no_letter</th>\n",
       "      <td>0.340659</td>\n",
       "      <td>0.901099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical-chatgpt_no_letter</th>\n",
       "      <td>0.197802</td>\n",
       "      <td>0.791209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medfound7B_no_letter</th>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.219780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            # correct  # interpretable\n",
       "meditron3-8B-prompt1         0.329670         0.978022\n",
       "huatuo-o1-prompt1            0.351648         1.000000\n",
       "huatuo-o1-prompt2            0.351648         1.000000\n",
       "diabetica-o1-prompt1         0.329670         1.000000\n",
       "diabetica-o1-prompt2         0.340659         1.000000\n",
       "diabetica-7B-prompt1         0.296703         0.989011\n",
       "diabetica-7B-prompt2         0.318681         1.000000\n",
       "medfound7B-prompt1           0.043956         0.186813\n",
       "medfound7B-prompt2           0.120879         0.384615\n",
       "clinical-chatgpt-prompt1     0.197802         0.791209\n",
       "clinical-chatgpt-prompt2     0.197802         0.945055\n",
       "huatuo-o1_no_letter          0.329670         0.967033\n",
       "diabetica-o1_no_letter       0.274725         0.945055\n",
       "diabetica-7B_no_letter       0.274725         0.879121\n",
       "meditron3-8B_no_letter       0.340659         0.901099\n",
       "clinical-chatgpt_no_letter   0.197802         0.791209\n",
       "medfound7B_no_letter         0.043956         0.219780"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results/n_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1ba312c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'huatuo-o1_prompt': ['huatuo-o1-prompt1', 'huatuo-o1-prompt2'],\n",
       " 'huatuo-o1_token': ['huatuo-o1-prompt1', 'huatuo-o1_no_letter'],\n",
       " 'diabetica-o1_prompt': ['diabetica-o1-prompt1', 'diabetica-o1-prompt2'],\n",
       " 'diabetica-o1_token': ['diabetica-o1-prompt1', 'diabetica-o1_no_letter'],\n",
       " 'diabetica-7B_prompt': ['diabetica-7B-prompt1', 'diabetica-7B-prompt2'],\n",
       " 'diabetica-7B_token': ['diabetica-7B-prompt1', 'diabetica-7B_no_letter'],\n",
       " 'meditron3-8B_prompt': ['meditron3-8B-prompt1', 'meditron3-8B-prompt2'],\n",
       " 'meditron3-8B_token': ['meditron3-8B-prompt1', 'meditron3-8B_no_letter'],\n",
       " 'clinical-chatgpt_prompt': ['clinical-chatgpt-prompt1',\n",
       "  'clinical-chatgpt-prompt2'],\n",
       " 'clinical-chatgpt_token': ['clinical-chatgpt-prompt1',\n",
       "  'clinical-chatgpt_no_letter'],\n",
       " 'medfound7B_prompt': ['medfound7B-prompt1', 'medfound7B-prompt2'],\n",
       " 'medfound7B_token': ['medfound7B-prompt1', 'medfound7B_no_letter']}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = {}\n",
    "for m_ in model_paths_no_token.keys():\n",
    "    groups[f\"{m_}_prompt\"] = [f\"{m_}-prompt1\", f\"{m_}-prompt2\"]\n",
    "    groups[f\"{m_}_token\"] = [f\"{m_}-prompt1\", f\"{m_}_no_letter\"]\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "41cb1503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUATUO-O1_PROMPT\n",
      "\n",
      "    58 agreements, \n",
      "    # match & correct = 23.000, \n",
      "    match-rate = 0.64, 95% CI [0.53, 0.73],\n",
      "    Cohen's coef = 0.5467032967032966,\n",
      "    Bootstrap CI Cohen's = (np.float64(0.423076923076923), np.float64(0.6703296703296704))\n",
      "\n",
      "\n",
      "HUATUO-O1_TOKEN\n",
      "\n",
      "    44 agreements, \n",
      "    # match & correct = 19.000, \n",
      "    match-rate = 0.48, 95% CI [0.38, 0.58],\n",
      "    Cohen's coef = 0.35439560439560436,\n",
      "    Bootstrap CI Cohen's = (np.float64(0.23076923076923078), np.float64(0.49175824175824173))\n",
      "\n",
      "\n",
      "DIABETICA-O1_PROMPT\n",
      "\n",
      "    47 agreements, \n",
      "    # match & correct = 17.000, \n",
      "    match-rate = 0.52, 95% CI [0.42, 0.62],\n",
      "    Cohen's coef = 0.39560439560439564,\n",
      "    Bootstrap CI Cohen's = (np.float64(0.27197802197802196), np.float64(0.5192307692307693))\n",
      "\n",
      "\n",
      "DIABETICA-O1_TOKEN\n",
      "\n",
      "    44 agreements, \n",
      "    # match & correct = 16.000, \n",
      "    match-rate = 0.48, 95% CI [0.38, 0.58],\n",
      "    Cohen's coef = 0.35439560439560436,\n",
      "    Bootstrap CI Cohen's = (np.float64(0.23076923076923078), np.float64(0.47802197802197804))\n",
      "\n",
      "\n",
      "DIABETICA-7B_PROMPT\n",
      "\n",
      "    64 agreements, \n",
      "    # match & correct = 23.000, \n",
      "    match-rate = 0.70, 95% CI [0.60, 0.79],\n",
      "    Cohen's coef = 0.6291208791208791,\n",
      "    Bootstrap CI Cohen's = (np.float64(0.5054945054945054), np.float64(0.7390109890109888))\n",
      "\n",
      "\n",
      "DIABETICA-7B_TOKEN\n",
      "\n",
      "    46 agreements, \n",
      "    # match & correct = 16.000, \n",
      "    match-rate = 0.51, 95% CI [0.40, 0.61],\n",
      "    Cohen's coef = 0.3818681318681318,\n",
      "    Bootstrap CI Cohen's = (np.float64(0.2582417582417582), np.float64(0.5054945054945054))\n",
      "\n",
      "\n",
      "MEDITRON3-8B_TOKEN\n",
      "\n",
      "    32 agreements, \n",
      "    # match & correct = 15.000, \n",
      "    match-rate = 0.35, 95% CI [0.26, 0.45],\n",
      "    Cohen's coef = 0.18956043956043958,\n",
      "    Bootstrap CI Cohen's = (np.float64(0.06593406593406591), np.float64(0.3131868131868132))\n",
      "\n",
      "\n",
      "CLINICAL-CHATGPT_PROMPT\n",
      "\n",
      "    29 agreements, \n",
      "    # match & correct = 6.000, \n",
      "    match-rate = 0.32, 95% CI [0.23, 0.42],\n",
      "    Cohen's coef = 0.1483516483516483,\n",
      "    Bootstrap CI Cohen's = (np.float64(0.024725274725274707), np.float64(0.27197802197802196))\n",
      "\n",
      "\n",
      "CLINICAL-CHATGPT_TOKEN\n",
      "\n",
      "    31 agreements, \n",
      "    # match & correct = 10.000, \n",
      "    match-rate = 0.34, 95% CI [0.25, 0.44],\n",
      "    Cohen's coef = 0.1758241758241758,\n",
      "    Bootstrap CI Cohen's = (np.float64(0.052197802197802186), np.float64(0.2994505494505494))\n",
      "\n",
      "\n",
      "MEDFOUND7B_PROMPT\n",
      "\n",
      "    5 agreements, \n",
      "    # match & correct = 1.000, \n",
      "    match-rate = 0.05, 95% CI [0.02, 0.12],\n",
      "    Cohen's coef = -0.1813186813186813,\n",
      "    Bootstrap CI Cohen's = (np.float64(-0.23626373626373628), np.float64(-0.11263736263736265))\n",
      "\n",
      "\n",
      "MEDFOUND7B_TOKEN\n",
      "\n",
      "    4 agreements, \n",
      "    # match & correct = 2.000, \n",
      "    match-rate = 0.04, 95% CI [0.02, 0.11],\n",
      "    Cohen's coef = -0.19505494505494506,\n",
      "    Bootstrap CI Cohen's = (np.float64(-0.23626373626373628), np.float64(-0.1401098901098901))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_kappa(x, y, B=10000, n=n_cases):\n",
    "    # bootstrap for Cohen's coefficient confidence intervals\n",
    "    rng = np.random.default_rng(seed=np.random.choice(100000))\n",
    "    idx = np.arange(n)\n",
    "    kappas = []\n",
    "    for b in range(B): \n",
    "        I = rng.choice(idx, len(idx), True)\n",
    "        tmp_agreement_mask = (x == y) & x.notna() & y.notna()\n",
    "        tmp_n_agree = tmp_agreement_mask.iloc[I].sum()\n",
    "        kappas.append((tmp_n_agree/n-0.2)/0.8)\n",
    "    lo, hi = np.percentile(kappas, [2.5, 97.5])\n",
    "    return lo, hi\n",
    "\n",
    "\n",
    "accuracy_when_agree = {}\n",
    "\n",
    "    \n",
    "for model, cols in groups.items():\n",
    "    if len(cols) == 2:\n",
    "        col1, col2 = cols\n",
    "        if not (col1 in df_all.columns and col2 in df_all.columns):\n",
    "            continue\n",
    "            \n",
    "        # Keep rows where both predictions agree and are not NaN\n",
    "        agreement_mask = (df_all[col1] == df_all[col2]) & df_all[col1].notna() & df_all[col2].notna()\n",
    "        agreed_preds = df_all.loc[agreement_mask, col1]\n",
    "        truths = df_all.loc[agreement_mask, 'truth']\n",
    "        x, y = df_all[col1], df_all[col2]\n",
    "        low, high = bootstrap_kappa(x, y)\n",
    "        # Calculate accuracy where predictions agree\n",
    "        acc = (agreed_preds == truths).sum()\n",
    "        accuracy_when_agree[model] = {\n",
    "            \"n_agreements\": agreement_mask.sum(),\n",
    "            \"# match & correct\": acc,\n",
    "            \"bootstrap CI\": (low, high)\n",
    "        }\n",
    "\n",
    "# Display results\n",
    "for model, stats in accuracy_when_agree.items():\n",
    "    lo, hi = proportion_confint(stats['n_agreements'], n, method=\"wilson\")\n",
    "    print(f\"\"\"{model.upper()}\\n\n",
    "    {stats['n_agreements']} agreements, \n",
    "    # match & correct = {stats['# match & correct']:.3f}, \n",
    "    match-rate = {stats['n_agreements']/n_cases:.2f}, 95% CI [{lo:.2f}, {hi:.2f}],\n",
    "    Cohen's coef = {(stats['n_agreements']/n_cases-0.2)/0.8},\n",
    "    Bootstrap CI Cohen's = {stats['bootstrap CI']}\\n\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c06d37d5-4447-4ff8-b34b-8ab76da9a426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># correct</th>\n",
       "      <th># interpretable</th>\n",
       "      <th>acc</th>\n",
       "      <th>lo_prop</th>\n",
       "      <th>hi_prop</th>\n",
       "      <th>lo_count</th>\n",
       "      <th>hi_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>meditron3-8B-prompt1</th>\n",
       "      <td>30</td>\n",
       "      <td>89</td>\n",
       "      <td>0.329670</td>\n",
       "      <td>0.241709</td>\n",
       "      <td>0.431430</td>\n",
       "      <td>21.995519</td>\n",
       "      <td>39.260105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huatuo-o1-prompt1</th>\n",
       "      <td>32</td>\n",
       "      <td>91</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.261373</td>\n",
       "      <td>0.453942</td>\n",
       "      <td>23.784923</td>\n",
       "      <td>41.308685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huatuo-o1-prompt2</th>\n",
       "      <td>32</td>\n",
       "      <td>91</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.261373</td>\n",
       "      <td>0.453942</td>\n",
       "      <td>23.784923</td>\n",
       "      <td>41.308685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-o1-prompt1</th>\n",
       "      <td>30</td>\n",
       "      <td>91</td>\n",
       "      <td>0.329670</td>\n",
       "      <td>0.241709</td>\n",
       "      <td>0.431430</td>\n",
       "      <td>21.995519</td>\n",
       "      <td>39.260105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-o1-prompt2</th>\n",
       "      <td>31</td>\n",
       "      <td>91</td>\n",
       "      <td>0.340659</td>\n",
       "      <td>0.251514</td>\n",
       "      <td>0.442713</td>\n",
       "      <td>22.887746</td>\n",
       "      <td>40.286870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-7B-prompt1</th>\n",
       "      <td>27</td>\n",
       "      <td>90</td>\n",
       "      <td>0.296703</td>\n",
       "      <td>0.212635</td>\n",
       "      <td>0.397240</td>\n",
       "      <td>19.349776</td>\n",
       "      <td>36.148872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-7B-prompt2</th>\n",
       "      <td>29</td>\n",
       "      <td>91</td>\n",
       "      <td>0.318681</td>\n",
       "      <td>0.231960</td>\n",
       "      <td>0.420091</td>\n",
       "      <td>21.108360</td>\n",
       "      <td>38.228272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medfound7B-prompt1</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.017224</td>\n",
       "      <td>0.107631</td>\n",
       "      <td>1.567417</td>\n",
       "      <td>9.794415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medfound7B-prompt2</th>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.068855</td>\n",
       "      <td>0.203615</td>\n",
       "      <td>6.265813</td>\n",
       "      <td>18.528964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical-chatgpt-prompt1</th>\n",
       "      <td>18</td>\n",
       "      <td>72</td>\n",
       "      <td>0.197802</td>\n",
       "      <td>0.128945</td>\n",
       "      <td>0.291140</td>\n",
       "      <td>11.733959</td>\n",
       "      <td>26.493762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical-chatgpt-prompt2</th>\n",
       "      <td>18</td>\n",
       "      <td>86</td>\n",
       "      <td>0.197802</td>\n",
       "      <td>0.128945</td>\n",
       "      <td>0.291140</td>\n",
       "      <td>11.733959</td>\n",
       "      <td>26.493762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huatuo-o1_no_letter</th>\n",
       "      <td>30</td>\n",
       "      <td>88</td>\n",
       "      <td>0.329670</td>\n",
       "      <td>0.241709</td>\n",
       "      <td>0.431430</td>\n",
       "      <td>21.995519</td>\n",
       "      <td>39.260105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-o1_no_letter</th>\n",
       "      <td>25</td>\n",
       "      <td>86</td>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.193552</td>\n",
       "      <td>0.374148</td>\n",
       "      <td>17.613212</td>\n",
       "      <td>34.047452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-7B_no_letter</th>\n",
       "      <td>25</td>\n",
       "      <td>80</td>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.193552</td>\n",
       "      <td>0.374148</td>\n",
       "      <td>17.613212</td>\n",
       "      <td>34.047452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meditron3-8B_no_letter</th>\n",
       "      <td>31</td>\n",
       "      <td>82</td>\n",
       "      <td>0.340659</td>\n",
       "      <td>0.251514</td>\n",
       "      <td>0.442713</td>\n",
       "      <td>22.887746</td>\n",
       "      <td>40.286870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical-chatgpt_no_letter</th>\n",
       "      <td>18</td>\n",
       "      <td>72</td>\n",
       "      <td>0.197802</td>\n",
       "      <td>0.128945</td>\n",
       "      <td>0.291140</td>\n",
       "      <td>11.733959</td>\n",
       "      <td>26.493762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medfound7B_no_letter</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.017224</td>\n",
       "      <td>0.107631</td>\n",
       "      <td>1.567417</td>\n",
       "      <td>9.794415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            # correct  # interpretable       acc   lo_prop  \\\n",
       "meditron3-8B-prompt1               30               89  0.329670  0.241709   \n",
       "huatuo-o1-prompt1                  32               91  0.351648  0.261373   \n",
       "huatuo-o1-prompt2                  32               91  0.351648  0.261373   \n",
       "diabetica-o1-prompt1               30               91  0.329670  0.241709   \n",
       "diabetica-o1-prompt2               31               91  0.340659  0.251514   \n",
       "diabetica-7B-prompt1               27               90  0.296703  0.212635   \n",
       "diabetica-7B-prompt2               29               91  0.318681  0.231960   \n",
       "medfound7B-prompt1                  4               17  0.043956  0.017224   \n",
       "medfound7B-prompt2                 11               35  0.120879  0.068855   \n",
       "clinical-chatgpt-prompt1           18               72  0.197802  0.128945   \n",
       "clinical-chatgpt-prompt2           18               86  0.197802  0.128945   \n",
       "huatuo-o1_no_letter                30               88  0.329670  0.241709   \n",
       "diabetica-o1_no_letter             25               86  0.274725  0.193552   \n",
       "diabetica-7B_no_letter             25               80  0.274725  0.193552   \n",
       "meditron3-8B_no_letter             31               82  0.340659  0.251514   \n",
       "clinical-chatgpt_no_letter         18               72  0.197802  0.128945   \n",
       "medfound7B_no_letter                4               20  0.043956  0.017224   \n",
       "\n",
       "                             hi_prop   lo_count   hi_count  \n",
       "meditron3-8B-prompt1        0.431430  21.995519  39.260105  \n",
       "huatuo-o1-prompt1           0.453942  23.784923  41.308685  \n",
       "huatuo-o1-prompt2           0.453942  23.784923  41.308685  \n",
       "diabetica-o1-prompt1        0.431430  21.995519  39.260105  \n",
       "diabetica-o1-prompt2        0.442713  22.887746  40.286870  \n",
       "diabetica-7B-prompt1        0.397240  19.349776  36.148872  \n",
       "diabetica-7B-prompt2        0.420091  21.108360  38.228272  \n",
       "medfound7B-prompt1          0.107631   1.567417   9.794415  \n",
       "medfound7B-prompt2          0.203615   6.265813  18.528964  \n",
       "clinical-chatgpt-prompt1    0.291140  11.733959  26.493762  \n",
       "clinical-chatgpt-prompt2    0.291140  11.733959  26.493762  \n",
       "huatuo-o1_no_letter         0.431430  21.995519  39.260105  \n",
       "diabetica-o1_no_letter      0.374148  17.613212  34.047452  \n",
       "diabetica-7B_no_letter      0.374148  17.613212  34.047452  \n",
       "meditron3-8B_no_letter      0.442713  22.887746  40.286870  \n",
       "clinical-chatgpt_no_letter  0.291140  11.733959  26.493762  \n",
       "medfound7B_no_letter        0.107631   1.567417   9.794415  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_ci(k, n=n_cases):\n",
    "    lo, hi = proportion_confint(k, n, method=\"wilson\")\n",
    "    return pd.Series({\n",
    "        \"acc\": k / n,\n",
    "        \"lo_prop\": lo,\n",
    "        \"hi_prop\": hi,\n",
    "        \"lo_count\": lo * n,\n",
    "        \"hi_count\": hi * n\n",
    "    })\n",
    "\n",
    "new = results[\"# correct\"].copy()\n",
    "# Apply across the DataFrame\n",
    "new = new.apply(count_ci)\n",
    "\n",
    "# Combine back with your model labels\n",
    "df_ci = pd.concat([results, new], axis=1)\n",
    "df_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef4cf0",
   "metadata": {},
   "source": [
    "In the following, we show the case index where we had agreement across prompt A, prompt B, and prompt A without letter token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a5d93251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huatuo-o1\n",
      "[np.int64(0), np.int64(6), np.int64(8), np.int64(21), np.int64(24), np.int64(38), np.int64(40), np.int64(54), np.int64(55), np.int64(56), np.int64(65), np.int64(68), np.int64(69), np.int64(71), np.int64(75), np.int64(83), np.int64(90)]\n",
      "\n",
      "\n",
      "diabetica-o1\n",
      "[np.int64(6), np.int64(20), np.int64(24), np.int64(40), np.int64(50), np.int64(59), np.int64(62), np.int64(68), np.int64(69), np.int64(77), np.int64(89), np.int64(90)]\n",
      "\n",
      "\n",
      "diabetica-7B\n",
      "[np.int64(6), np.int64(15), np.int64(19), np.int64(22), np.int64(34), np.int64(35), np.int64(40), np.int64(55), np.int64(59), np.int64(62), np.int64(68), np.int64(69), np.int64(83), np.int64(85), np.int64(89)]\n",
      "\n",
      "\n",
      "clinical-chatgpt\n",
      "[np.int64(5), np.int64(23), np.int64(40), np.int64(43), np.int64(71)]\n",
      "\n",
      "\n",
      "medfound7B\n",
      "[np.int64(71)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m_ in model_paths_no_token.keys():\n",
    "    acc_pmt1 = np.where(accuracy_df[f'{m_}-prompt1'])[0]\n",
    "    acc_no_let = np.where(accuracy_df[f'{m_}_no_letter'])[0]\n",
    "    if m_ != 'meditron3-8B':\n",
    "        acc_pmt2 = np.where(accuracy_df[f'{m_}-prompt2'])[0]\n",
    "        print(m_)\n",
    "        print(sorted(list((set(acc_pmt2).intersection(set(acc_pmt1))).intersection(set(acc_no_let)))))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4039dfdb-9191-4ea1-85d8-fe15de9c7a14",
   "metadata": {},
   "source": [
    "## McNemar's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fb105a6d-8237-4e99-90b6-d6165d97aec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huatuo-o1-prompt1 vs huatuo-o1-prompt2: p=1.0000\n",
      "huatuo-o1-prompt1 vs huatuo-o1_no_letter: p=0.8388\n",
      "huatuo-o1-prompt2 vs huatuo-o1_no_letter: p=0.8318\n",
      "\n",
      "\n",
      "diabetica-7B-prompt1 vs diabetica-7B-prompt2: p=0.7539\n",
      "diabetica-7B-prompt1 vs diabetica-7B_no_letter: p=0.8238\n",
      "diabetica-7B-prompt2 vs diabetica-7B_no_letter: p=0.5034\n",
      "\n",
      "\n",
      "diabetica-o1-prompt1 vs diabetica-o1-prompt2: p=1.0000\n",
      "diabetica-o1-prompt1 vs diabetica-o1_no_letter: p=0.4049\n",
      "diabetica-o1-prompt2 vs diabetica-o1_no_letter: p=0.3075\n",
      "\n",
      "\n",
      "meditron3-8B-prompt1 vs meditron3-8B_no_letter: p=1.0000\n",
      "\n",
      "\n",
      "clinical-chatgpt-prompt1 vs clinical-chatgpt-prompt2: p=1.0000\n",
      "clinical-chatgpt-prompt1 vs clinical-chatgpt_no_letter: p=1.0000\n",
      "clinical-chatgpt-prompt2 vs clinical-chatgpt_no_letter: p=1.0000\n",
      "\n",
      "\n",
      "medfound7B-prompt1 vs medfound7B-prompt2: p=0.0923\n",
      "medfound7B-prompt1 vs medfound7B_no_letter: p=1.0000\n",
      "medfound7B-prompt2 vs medfound7B_no_letter: p=0.0654\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# McNemar's test\n",
    "def mcnemar_single(series1_bm, series2_bm): \n",
    "    x, y = series1_bm.to_numpy(), series2_bm.to_numpy()\n",
    "    n10 = np.sum((x==1)&(y==0))\n",
    "    n01 = np.sum((x==0)&(y==1))\n",
    "    table = [[np.sum((x==0)&(y==0)), n01],\n",
    "             [n10, np.sum((x==1)&(y==1))]]\n",
    "    pvals = mcnemar(table, exact=True).pvalue\n",
    "    return pvals\n",
    "\n",
    "    \n",
    "for cols in models: \n",
    "    pairs = list(itertools.combinations(cols, 2))\n",
    "    for a, b in pairs:\n",
    "        p = mcnemar_single(accuracy_df[a], accuracy_df[b])\n",
    "        print(f\"{a} vs {b}: p={p:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f0473-545a-44ac-8a1f-ec7f4fa10a12",
   "metadata": {},
   "source": [
    "## Stuart-Maxwell Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "00f87211-6dc6-4f9d-bc91-f03b57719319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huatuo-o1\n",
      "huatuo-o1-prompt1 vs huatuo-o1-prompt2: p=9.992007221626409e-16\n",
      "huatuo-o1-prompt1 vs huatuo-o1_no_letter: p=2.645000279777321e-05\n",
      "huatuo-o1-prompt2 vs huatuo-o1_no_letter: p=1.170857699683836e-08\n",
      "\n",
      "\n",
      "diabetica-7B\n",
      "diabetica-7B-prompt1 vs diabetica-7B-prompt2: p=0.0\n",
      "diabetica-7B-prompt1 vs diabetica-7B_no_letter: p=3.2980825848127893e-09\n",
      "diabetica-7B-prompt2 vs diabetica-7B_no_letter: p=1.8207657603852567e-13\n",
      "\n",
      "\n",
      "diabetica-o1\n",
      "diabetica-o1-prompt1 vs diabetica-o1-prompt2: p=2.421497805160655e-06\n",
      "diabetica-o1-prompt1 vs diabetica-o1_no_letter: p=1.949451960669979e-05\n",
      "diabetica-o1-prompt2 vs diabetica-o1_no_letter: p=4.433931888314646e-09\n",
      "\n",
      "\n",
      "meditron3-8B\n",
      "meditron3-8B-prompt1 vs meditron3-8B_no_letter: p=0.005977985631178462\n",
      "\n",
      "\n",
      "clinical-chatgpt\n",
      "clinical-chatgpt-prompt1 vs clinical-chatgpt-prompt2: p=0.1898673855785693\n",
      "clinical-chatgpt-prompt1 vs clinical-chatgpt_no_letter: p=0.004637283163608008\n",
      "clinical-chatgpt-prompt2 vs clinical-chatgpt_no_letter: p=0.342828844242973\n",
      "\n",
      "\n",
      "medfound7B\n",
      "medfound7B-prompt1 vs medfound7B-prompt2: p=nan\n",
      "medfound7B-prompt1 vs medfound7B_no_letter: p=0.717142446980161\n",
      "medfound7B-prompt2 vs medfound7B_no_letter: p=0.4440083435776955\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def stuart_maxwell(x, y): \n",
    "    tab = pd.crosstab(x, y)\n",
    "    res = Table(tab).test_nominal_association()  # Stuart–Maxwell\n",
    "    return res.statistic, res.pvalue\n",
    "    \n",
    "for cols in models: \n",
    "    print(cols[0].split('-prompt')[0])\n",
    "    pairs = list(itertools.combinations(cols, 2))\n",
    "    pvals = []\n",
    "    for a,b in pairs:\n",
    "        stats, pval = stuart_maxwell(df_all[a], df_all[b])\n",
    "        print(f\"{a} vs {b}: p={pval}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7dd49e2f-61a6-4f39-8537-e783a6c03cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># correct</th>\n",
       "      <th># interpretable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>meditron3-8B-prompt1</th>\n",
       "      <td>0.329670</td>\n",
       "      <td>0.978022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huatuo-o1-prompt1</th>\n",
       "      <td>0.351648</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huatuo-o1-prompt2</th>\n",
       "      <td>0.351648</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-o1-prompt1</th>\n",
       "      <td>0.329670</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-o1-prompt2</th>\n",
       "      <td>0.340659</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-7B-prompt1</th>\n",
       "      <td>0.296703</td>\n",
       "      <td>0.989011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-7B-prompt2</th>\n",
       "      <td>0.318681</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medfound7B-prompt1</th>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.186813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medfound7B-prompt2</th>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical-chatgpt-prompt1</th>\n",
       "      <td>0.197802</td>\n",
       "      <td>0.791209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical-chatgpt-prompt2</th>\n",
       "      <td>0.197802</td>\n",
       "      <td>0.945055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huatuo-o1_no_letter</th>\n",
       "      <td>0.329670</td>\n",
       "      <td>0.967033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-o1_no_letter</th>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.945055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetica-7B_no_letter</th>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.879121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meditron3-8B_no_letter</th>\n",
       "      <td>0.340659</td>\n",
       "      <td>0.901099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical-chatgpt_no_letter</th>\n",
       "      <td>0.197802</td>\n",
       "      <td>0.791209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medfound7B_no_letter</th>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.219780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            # correct  # interpretable\n",
       "meditron3-8B-prompt1         0.329670         0.978022\n",
       "huatuo-o1-prompt1            0.351648         1.000000\n",
       "huatuo-o1-prompt2            0.351648         1.000000\n",
       "diabetica-o1-prompt1         0.329670         1.000000\n",
       "diabetica-o1-prompt2         0.340659         1.000000\n",
       "diabetica-7B-prompt1         0.296703         0.989011\n",
       "diabetica-7B-prompt2         0.318681         1.000000\n",
       "medfound7B-prompt1           0.043956         0.186813\n",
       "medfound7B-prompt2           0.120879         0.384615\n",
       "clinical-chatgpt-prompt1     0.197802         0.791209\n",
       "clinical-chatgpt-prompt2     0.197802         0.945055\n",
       "huatuo-o1_no_letter          0.329670         0.967033\n",
       "diabetica-o1_no_letter       0.274725         0.945055\n",
       "diabetica-7B_no_letter       0.274725         0.879121\n",
       "meditron3-8B_no_letter       0.340659         0.901099\n",
       "clinical-chatgpt_no_letter   0.197802         0.791209\n",
       "medfound7B_no_letter         0.043956         0.219780"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results/ n_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821c764b-d756-4abd-9ce0-0c6c64d8177e",
   "metadata": {},
   "source": [
    "## Numerical Reproducibility: RTX 5090 (CUDA 12.8) and comparison against L40S (CUDA 11.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6fbb3179-c662-4421-8d83-59077d3a4339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huatuo-o1-prompt1    0.395604\n",
      "dtype: float64\n",
      "diabetica-o1-prompt1    0.307692\n",
      "dtype: float64\n",
      "diabetica-7B-prompt1    0.307692\n",
      "dtype: float64\n",
      "meditron3-8B-prompt1    0.340659\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# reproducibility \n",
    "\n",
    "path = \"./../results/MACHINE2\"\n",
    "model_rep = ['huatuo-o1', 'diabetica-o1', 'diabetica-7B', 'meditron3-8B']\n",
    "\n",
    "for i_, m in enumerate(model_rep):\n",
    "    tmp = pd.read_excel(os.path.join(path, f\"{m}_promptID_001_output_0.xlsx\"), index_col=0, header=None)\n",
    "    if i_ == 0:\n",
    "        df_machine2 = tmp\n",
    "    else:\n",
    "        df_machine2 = pd.concat([df_machine2, tmp], axis=1)\n",
    "df_machine2 = pd.concat([df_machine2, df_all[\"truth\"]], axis=1)\n",
    "\n",
    "df_machine2.columns = [[f\"{m}-prompt1\" for m in model_rep] + [\"truth\"]]\n",
    "\n",
    "# here we keep the value in entry if is one among A, B, C, D, or E\n",
    "# we report as NaN otherwise\n",
    "for col in df_machine2.columns[:-1]:\n",
    "    df_machine2[col] = df_machine2[col].where(df_machine2[col].isin(options), np.nan)\n",
    "\n",
    "for m in model_rep:\n",
    "    accuracy_machine2 = df_machine2[f\"{m}-prompt1\"].eq(df_all['truth'], axis=0)\n",
    "    print(accuracy_machine2.sum() / n_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "54a69e5c-88ce-4618-a195-04afe644dedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>huatuo-o1-prompt1</th>\n",
       "      <th>diabetica-o1-prompt1</th>\n",
       "      <th>diabetica-7B-prompt1</th>\n",
       "      <th>meditron3-8B-prompt1</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   huatuo-o1-prompt1 diabetica-o1-prompt1 diabetica-7B-prompt1  \\\n",
       "1                  B                    C                    A   \n",
       "2                  D                    B                    D   \n",
       "5                  B                    B                    B   \n",
       "7                  C                    C                    C   \n",
       "8                  A                    A                    C   \n",
       "..               ...                  ...                  ...   \n",
       "95                 B                    D                    D   \n",
       "96                 D                    D                    E   \n",
       "97                 D                    B                    D   \n",
       "98                 D                    B                    D   \n",
       "99                 B                    B                    B   \n",
       "\n",
       "   meditron3-8B-prompt1 truth  \n",
       "1                     B     B  \n",
       "2                   NaN     A  \n",
       "5                     A     C  \n",
       "7                   NaN     A  \n",
       "8                     A     E  \n",
       "..                  ...   ...  \n",
       "95                    D     E  \n",
       "96                    E     E  \n",
       "97                    D     A  \n",
       "98                    D     D  \n",
       "99                    A     B  \n",
       "\n",
       "[91 rows x 5 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_machine2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867d7a7-5822-4fee-8dad-ce6ab0367396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "539d6feb-14eb-46f1-9290-ce9732fccb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huatuo-o1-prompt1\n",
      "diabetica-o1-prompt1\n",
      "diabetica-7B-prompt1\n",
      "meditron3-8B-prompt1\n",
      "HUATUO-O1-PROMPT1\n",
      "\n",
      "    55 agreements, \n",
      "    accuracy model = huatuo-o1-prompt1    0.395604\n",
      "dtype: float64, \n",
      "    CI accuracy = (huatuo-o1-prompt1    0.301332\n",
      "dtype: float64, huatuo-o1-prompt1    0.498334\n",
      "dtype: float64), \n",
      "    Match rate = 0.60, 95% CI [0.50, 0.70],\n",
      "    cohens = 0.5054945054945054,\n",
      "    bootstrap k = (np.float64(0.3818681318681318), np.float64(0.6291208791208791)),\n",
      "    p-val McNemar = 0.5034446716308594,\n",
      "    stats, p-val Stuart-Maxwell = (np.float64(96.03220116739136), np.float64(1.9095836023552692e-13))\n",
      "\n",
      "\n",
      "DIABETICA-O1-PROMPT1\n",
      "\n",
      "    57 agreements, \n",
      "    accuracy model = diabetica-o1-prompt1    0.307692\n",
      "dtype: float64, \n",
      "    CI accuracy = (diabetica-o1-prompt1    0.222268\n",
      "dtype: float64, diabetica-o1-prompt1    0.408695\n",
      "dtype: float64), \n",
      "    Match rate = 0.63, 95% CI [0.52, 0.72],\n",
      "    cohens = 0.532967032967033,\n",
      "    bootstrap k = (np.float64(0.40934065934065933), np.float64(0.6565934065934065)),\n",
      "    p-val McNemar = 0.803619384765625,\n",
      "    stats, p-val Stuart-Maxwell = (np.float64(95.3843627227876), np.float64(2.5202062658991053e-13))\n",
      "\n",
      "\n",
      "DIABETICA-7B-PROMPT1\n",
      "\n",
      "    68 agreements, \n",
      "    accuracy model = diabetica-7B-prompt1    0.307692\n",
      "dtype: float64, \n",
      "    CI accuracy = (diabetica-7B-prompt1    0.222268\n",
      "dtype: float64, diabetica-7B-prompt1    0.408695\n",
      "dtype: float64), \n",
      "    Match rate = 0.75, 95% CI [0.65, 0.83],\n",
      "    cohens = 0.6840659340659341,\n",
      "    bootstrap k = (np.float64(0.5741758241758241), np.float64(0.7939560439560439)),\n",
      "    p-val McNemar = 1.0,\n",
      "    stats, p-val Stuart-Maxwell = (np.float64(164.5649356748887), np.float64(0.0))\n",
      "\n",
      "\n",
      "MEDITRON3-8B-PROMPT1\n",
      "\n",
      "    41 agreements, \n",
      "    accuracy model = meditron3-8B-prompt1    0.340659\n",
      "dtype: float64, \n",
      "    CI accuracy = (meditron3-8B-prompt1    0.251514\n",
      "dtype: float64, meditron3-8B-prompt1    0.442713\n",
      "dtype: float64), \n",
      "    Match rate = 0.45, 95% CI [0.35, 0.55],\n",
      "    cohens = 0.3131868131868132,\n",
      "    bootstrap k = (np.float64(0.18956043956043958), np.float64(0.4368131868131868)),\n",
      "    p-val McNemar = 1.0,\n",
      "    stats, p-val Stuart-Maxwell = (np.float64(51.50787397244694), np.float64(1.3148539394713943e-05))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_when_agree = {}\n",
    "\n",
    "models_list = [f\"{m}-prompt1\" for m in model_rep]\n",
    "for model in models_list:\n",
    "    print(model)\n",
    "    # print(model)\n",
    "    # Keep rows where both predictions agree and are not NaN\n",
    "    count_correct = df_machine2[model].eq(df_all['truth'], axis=0).sum()\n",
    "\n",
    "    agreement_mask = (df_all[model] == df_machine2[model].squeeze()) & df_all[model].notna() & df_machine2[model].squeeze().notna()\n",
    "    agreed_preds = df_all.loc[agreement_mask, model]\n",
    "    truths = df_all.loc[agreement_mask, 'truth']\n",
    "    low, high = bootstrap_kappa(df_all[model], df_machine2[model].squeeze())\n",
    "    # Calculate accuracy where predictions agree\n",
    "    acc = (agreed_preds == truths).sum()\n",
    "\n",
    "    pval_mcnemar = mcnemar_single(df_machine2[model].squeeze().eq(df_all['truth'], axis=0), df_all[model].eq(df_all['truth'], axis=0))\n",
    "    stat, pval_stuart = stuartmaxwell(df_machine2[model].squeeze(), df_all[model])\n",
    "\n",
    "    accuracy_when_agree[model] = {\n",
    "        \"n_agreements\": agreement_mask.sum(),\n",
    "        \"accuracy\": count_correct/n_cases,\n",
    "        \"CI accuracy\":  count_ci(count_correct, n=n_cases)/ n_cases,\n",
    "        \"accuracy in agreement\": acc,\n",
    "        \"bootstrap CI\": (low, high),\n",
    "        \"p-val McNemar\": pval_mcnemar,\n",
    "        \"Stuart-Maxwell test\": (stat, pval_stuart)\n",
    "        }\n",
    "\n",
    "# Display results\n",
    "for model, stats in accuracy_when_agree.items():\n",
    "    lo, hi = proportion_confint(stats['n_agreements'], n, method=\"wilson\")\n",
    "    print(f\"\"\"{model.upper()}\\n\n",
    "    {stats['n_agreements']} agreements, \n",
    "    accuracy model = {(stats['accuracy'])}, \n",
    "    CI accuracy = {stats['CI accuracy']['lo_count'], stats['CI accuracy']['hi_count']}, \n",
    "    Match rate = {stats['n_agreements']/n_cases:.2f}, 95% CI [{lo:.2f}, {hi:.2f}],\n",
    "    cohens = {(stats['n_agreements']/n_cases-0.2)/0.8},\n",
    "    bootstrap k = {stats['bootstrap CI']},\n",
    "    p-val McNemar = {stats['p-val McNemar']},\n",
    "    stats, p-val Stuart-Maxwell = {stats['Stuart-Maxwell test']}\\n\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc3ed5-d87f-4027-94f5-406f8b2d3d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777d8c6-e1bd-4a18-8d4d-a229486139f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20f73573",
   "metadata": {},
   "source": [
    "# Experiment 3: Check Reasoning and Self-Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "physician_eval = {\"huatuo-o1_promptID_001_output_9\": [3, 4, 4, 4, 3, 2, 2, 3], \n",
    "                  \"huatuo-o1_promptID_002_output_11\": [2, 2, 2, 2],\n",
    "                  \"diabetica-o1_promptID_001_output_2\": [],\n",
    "                  \"diabetica-o1_promptID_002_output_3\": []}\n",
    "\n",
    "dict_fileid = {\"huatuo-o1\": {1: 9,\n",
    "                             2: 11},\n",
    "              \"diabetica-o1\": {1: 2,\n",
    "                               2: 3}\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e61935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table_dict = {}\n",
    "# we save a contigency table with frequency of selection of ESAP vs model response\n",
    "\n",
    "filtered_df_dict = {}\n",
    "# we save dataframes whenever the model prefers its previous response over ESAP\n",
    "\n",
    "missing_choice = {}\n",
    "\n",
    "for model_name in dict_fileid:\n",
    "    \n",
    "    path_to_reasoning = join(path_to_results, \"REASONING\", model_name)\n",
    "    contingency_table_dict[model_name] = {}\n",
    "    filtered_df_dict[model_name] = {}\n",
    "    missing_choice[model_name] = {}\n",
    "    \n",
    "    \n",
    "    for prompt, file_id in dict_fileid[model_name].items():\n",
    "        \n",
    "        experiment_name = f\"{model_name}_promptID_00{prompt}_output_{file_id}\"\n",
    "\n",
    "        list_df_esap = []\n",
    "        for esap in [1, 2]: # ground truth in first or second position\n",
    "\n",
    "            model_exp = {1: 2, 2: 1}[esap]  # model reasoning in other location\n",
    "            model_and_exp_name = experiment_name.split(\"_output\")[0]\n",
    "\n",
    "            filename = f\"reasoning_ESAP{esap}_{model_name}_{experiment_name}.xlsx\"\n",
    "            df_reason = pd.read_excel(join(path_to_reasoning, filename),\n",
    "                          header=None, index_col=0)\n",
    "            n_cols = df_reason.shape[1] \n",
    "            if n_cols >= 3: # eliminate nan columns\n",
    "                df_reason = df_reason.drop([i for i in range(3, n_cols+1)], axis=1)\n",
    "\n",
    "            df_reason.columns = [f\"ESAP {esap} preferred\", f\"Agreement {esap}\"]\n",
    "            df_reason[f\"ESAP {esap} preferred\"] = df_reason[f\"ESAP {esap} preferred\"].map({esap: True, model_exp: False})\n",
    "            list_df_esap.append(df_reason)\n",
    "            \n",
    "        df_ = pd.concat([pd.concat(list_df_esap, axis=1), accuracy_df[model_name+f\"-prompt{prompt}\"]], axis=1)\n",
    "\n",
    "        # df_[df_.columns[-1]] model response from experiment 1 was correct\n",
    "        df_subset = df_[~df_[df_.columns[-1]]] # model from experiment 1 was wrong\n",
    "\n",
    "        # Drop rows where 'ESAP preferred' is NaN\n",
    "        filtered_df = df_subset[(df_subset['ESAP 1 preferred'].notna()) \n",
    "                                & (df_subset['ESAP 2 preferred'].notna())]\n",
    "\n",
    "        # Count dropped rows\n",
    "        num_dropped = len(df_subset) - len(filtered_df)\n",
    "\n",
    "        # Create contingency table\n",
    "        contingency_table = pd.crosstab(filtered_df['ESAP 1 preferred'], filtered_df['ESAP 2 preferred'])\n",
    "        contingency_table_dict[model_name][prompt] = contingency_table\n",
    "        \n",
    "        filtered_df_dict[model_name][prompt] = filtered_df\n",
    "        \n",
    "        # print(f\"Rows with missing response in at least one of the two runs': {num_dropped}\")        \n",
    "        missing_choice[model_name][prompt] = num_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e4379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot_agreement_levels(dict_wrong_answers, model_name, prompt):\n",
    "    filtered_df = dict_wrong_answers[model_name][prompt]\n",
    "    prompt_label = [\"A\", \"B\"]\n",
    "\n",
    "    cond1 = filtered_df['ESAP 1 preferred'].astype(bool)\n",
    "    cond2 = filtered_df['ESAP 2 preferred'].astype(bool)\n",
    "\n",
    "    nn = filtered_df[(~cond1 & ~cond2)][['Agreement 1', 'Agreement 2']].mean(axis=1)\n",
    "    yy = filtered_df[(cond1 & cond2)][['Agreement 1', 'Agreement 2']].mean(axis=1)\n",
    "    n1y2 = filtered_df[(~cond1 & cond2)][['Agreement 1', 'Agreement 2']].mean(axis=1)\n",
    "    y1n2 = filtered_df[(cond1 & ~cond2)][['Agreement 1', 'Agreement 2']].mean(axis=1)\n",
    "\n",
    "    boxprops = dict(linewidth=2)\n",
    "    medianprops = dict(linewidth=2)\n",
    "    whiskerprops = dict(linewidth=2)\n",
    "    capprops = dict(linewidth=2)\n",
    "    flierprops = dict(marker='o', markersize=5, linestyle='none')\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=4, sharey=True)\n",
    "    ax[0].set_ylim([0,5.2])\n",
    "    for a in ax:\n",
    "        a.spines['bottom'].set_linewidth(3)\n",
    "        a.spines['left'].set_linewidth(3)\n",
    "        a.spines['top'].set_linewidth(3)\n",
    "        a.spines['right'].set_linewidth(3)\n",
    "        a.tick_params(width=3)\n",
    "\n",
    "    nn.plot.box(ax=ax[3], label='ESAP\\nnever',\n",
    "                boxprops=boxprops, medianprops=medianprops,\n",
    "                whiskerprops=whiskerprops, capprops=capprops, flierprops=flierprops,\n",
    "                title=f\"$n={nn.size}$\", fontsize=16)\n",
    "\n",
    "    yy.plot.box(ax=ax[0], label='ESAP\\nboth',\n",
    "                boxprops=boxprops, medianprops=medianprops,\n",
    "                whiskerprops=whiskerprops, capprops=capprops, flierprops=flierprops,\n",
    "                title=f\"$n={yy.size}$\", fontsize=14)\n",
    "    n1y2.plot.box(ax=ax[1], label='ESAP\\nsecond',\n",
    "                  boxprops=boxprops, medianprops=medianprops,\n",
    "                  whiskerprops=whiskerprops, capprops=capprops, flierprops=flierprops,\n",
    "                  title=f\"$n={n1y2.size}$\", fontsize=14)\n",
    "    y1n2.plot.box(ax=ax[2], label='ESAP\\nfirst', \n",
    "                  boxprops=boxprops, medianprops=medianprops,\n",
    "                  whiskerprops=whiskerprops, capprops=capprops, flierprops=flierprops,\n",
    "                  title=f\"$n={y1n2.size}$\", fontsize=14)\n",
    "    ax[0].set_ylabel(\"Agreement Level\", fontsize=18)\n",
    "\n",
    "    for i_ in range(4):\n",
    "        ax[i_].set_title(ax[i_].get_title(), fontsize=16)   # increase title size\n",
    "\n",
    "\n",
    "    fig.text(0.5, 0.99, f\"Comparing ESAP with \\nWrong Outcomes (Prompt {prompt_label[prompt-1]})\", \n",
    "             ha='center', fontsize=18);\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.15, top=0.9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.text(0.5, -0.05, \"Picking ESAP Explanation as Best\", ha='center', fontsize=20)\n",
    "    plt.savefig(f\"plots/{model_name}_{prompt}_boxplot.pdf\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380017c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in dict_fileid:\n",
    "    for prompt in dict_fileid[model_name]:\n",
    "        print(f\"{model_name} for prompt {prompt}\")\n",
    "        plot_boxplot_agreement_levels(filtered_df_dict, model_name, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table_dict['huatuo-o1'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9aa152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for each group (model + prompt)\n",
    "labels = ['A', 'B'] \n",
    "\n",
    "outcome = [\"Hall / None\",\n",
    "           \"ESAP \\nnever\",\n",
    "           \"ESAP \\nsecond\",\n",
    "           \"ESAP \\nfirst\",\n",
    "           \"ESAP \\nboth\"\n",
    "]\n",
    "\n",
    "data_dict = {}\n",
    "percentage_dict = {}\n",
    "\n",
    "for model_name in dict_fileid:\n",
    "    data_dict[model_name] = []\n",
    "    percentage_dict[model_name] = []\n",
    "    \n",
    "    for prompt in dict_fileid[model_name]:\n",
    "        tmp_data = np.append(missing_choice[model_name][prompt], \n",
    "                             np.ravel(contingency_table_dict[model_name][prompt].values))\n",
    "        data_dict[model_name].append(tmp_data)\n",
    "        percentage_dict[model_name].append(tmp_data / tmp_data.sum())\n",
    "\n",
    "    data_dict[model_name] = np.array(data_dict[model_name])\n",
    "    percentage_dict[model_name] = 100 * np.array(percentage_dict[model_name])\n",
    "\n",
    "# Define outcome colors (5 outcomes)\n",
    "colors = ['lightgray', 'orangered', 'gold', 'khaki', 'teal'] #  \n",
    "\n",
    "# Create plot\n",
    "nrows = len(dict_fileid.keys())\n",
    "fig, ax = plt.subplots(figsize=(10, 6), ncols=1, nrows=nrows, sharex=True)\n",
    "\n",
    "for j_, model_name in enumerate(dict_fileid.keys()):\n",
    "\n",
    "    ax[j_].spines['bottom'].set_linewidth(3)\n",
    "    ax[j_].spines['left'].set_linewidth(3)\n",
    "    ax[j_].spines['top'].set_linewidth(3)\n",
    "    ax[j_].spines['right'].set_linewidth(3)\n",
    "    ax[j_].tick_params(width=3)\n",
    "    \n",
    "    for k_, (percentage_, data_) in enumerate(zip(percentage_dict[model_name], data_dict[model_name])):\n",
    "\n",
    "        left = 0\n",
    "        for i in range(data_.size):  # for each outcome\n",
    "            value = data_[i]\n",
    "            perc = percentage_[i]\n",
    "            if perc > 2:  #  skip annotation on short segments\n",
    "                ax[j_].text(left + value / 2, k_, f\"{perc:.0f}%\", va='center', \n",
    "                                ha='center', fontsize=18, color='black')\n",
    "            \n",
    "            label = outcome[i] if (k_ == 0 and j_ == 0) else None\n",
    "            \n",
    "            ax[j_].barh(labels[k_], data_[i], left=left, color=colors[i], alpha=.7, \n",
    "                        label=label)\n",
    "            left += data_[i]\n",
    "\n",
    "        tick = ax[j_].yaxis.get_major_ticks()\n",
    "        for t in tick:\n",
    "            t.label1.set_fontsize(17) \n",
    "        \n",
    "        ax2 = ax[j_].secondary_yaxis('right')\n",
    "        \n",
    "        ax2.set_ylabel(f\"{model_file_name_dict[model_name]}\", rotation=270, labelpad=20, fontsize=17)\n",
    "        ax2.set_yticks([])\n",
    "        ax2.tick_params(length=0)    # hide its ticks if you’d like\n",
    "\n",
    "    tick = ax[1].xaxis.get_major_ticks()\n",
    "    for t in tick:\n",
    "        t.label1.set_fontsize(18) \n",
    "\n",
    "    # Add legend and labels\n",
    "    ax[1].set_xlabel('# Questions - Only Those Answered Incorrectly from Exp 1', fontsize=20)\n",
    "    # ax[1].set_xlim(0, 61)\n",
    "    ax[0].legend(bbox_to_anchor=(0.5, 1.4), loc='upper center', ncol=5, fontsize=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/model_re-eval_given_gt.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0705c314",
   "metadata": {},
   "source": [
    "# Experiment 2: Repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['HuatuoGPT-o1-8B', 'Diabetica-o1', 'Diabetica-7B', 'Meditron3-8B']\n",
    "model_file = ['huatuo-o1', 'diabetica-o1', 'diabetica-7B', 'meditron3-8B']\n",
    "temperature_list = [0.3, 0.6, 1.]\n",
    "path_repetitions = f\"{path_to_results}/REPETITIONS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75992876",
   "metadata": {},
   "source": [
    "The following comparison matrix contains the number of times the LLMs select the correct option. \n",
    "\n",
    "``rep_df`` is a Excel with 91 rows (cases) and 10 columns (repetitions).\n",
    "\n",
    "We repeat this across the three temperatures tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_frequency_string(arr):\n",
    "    \"\"\" This code returns nan if there is no majorty selected (e.g., two options with same\n",
    "    frequency). Otherwise, option and how many times it was selected. \"\"\"\n",
    "    if not arr.size:\n",
    "        return None, 0\n",
    "    unique_elements, counts = np.unique(arr, return_counts=True)\n",
    "    max_indexes = counts == np.max(counts)\n",
    "    if np.sum(max_indexes) > 1:\n",
    "        return np.nan, np.nan # returns not a number if we do not have a majority\n",
    "    # elif unique_elements[np.argmax(counts)] not in options:\n",
    "    #     return np.nan, np.nan # if max option is none\n",
    "    else:\n",
    "        return unique_elements[np.argmax(counts)], np.max(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860dbd8",
   "metadata": {},
   "source": [
    "Counting outputs: There are some cases where models does not make a selection, we report these values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the accuracy for each run, model, temperature\n",
    "bm_across_models_and_t = []\n",
    "comparison_mat_all = []\n",
    "most_sel_list_and_t = []\n",
    "count_sel_list_and_t = []\n",
    "\n",
    "for t_ in temperature_list:\n",
    "    bm_across_models = []\n",
    "    comparison_mat = []\n",
    "    most_sel_list = []\n",
    "    count_sel_list = []\n",
    "\n",
    "    for m_ in model_file: \n",
    "        # for each model and temperature we load the excel and retrieve information\n",
    "        rep_df = pd.read_excel(f\"{path_repetitions}/T={t_}/{m_}/{m_}_promptID_001_repetitions.xlsx\",\n",
    "                               header=None, index_col=0)\n",
    "        comparison = rep_df.eq(df_all['truth'], axis=0)\n",
    "        \n",
    "        most_sel = []\n",
    "        count_sel = []\n",
    "        for id_row, row in enumerate(rep_df.values):\n",
    "            row = row[~pd.isna(row)]\n",
    "            cat, count = max_frequency_string(row)\n",
    "            most_sel.append(cat)\n",
    "            count_sel.append(count)\n",
    "        most_sel_list.append(np.array(most_sel))\n",
    "        count_sel_list.append(np.array(count_sel))\n",
    "        \n",
    "        # how many times correct option selected across 10 runs\n",
    "        comparison_mat.append(comparison.sum(axis=1).values) \n",
    "        \n",
    "        # accuracy for each run\n",
    "        bm_across_models.append(comparison.sum(axis=0).values) \n",
    "        \n",
    "    most_sel_list_and_t.append(most_sel_list)\n",
    "    count_sel_list_and_t.append(count_sel_list)\n",
    "    comparison_mat_all.append(np.array(comparison_mat))\n",
    "    bm_across_models_and_t.append(bm_across_models)\n",
    "    \n",
    "comparison_mat = np.array(comparison_mat_all)\n",
    "most_sel_list_and_t = np.array(most_sel_list_and_t)\n",
    "count_sel_list_and_t = np.array(count_sel_list_and_t)\n",
    "\n",
    "indexes_questions = rep_df.index    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a209f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comparison_mat.shape) # accuracy (10/10 = 100%, 0/10 = 0%)\n",
    "print(most_sel_list_and_t.shape) # majority vote\n",
    "print(count_sel_list_and_t.shape) # frequency of majority vote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e9cc4",
   "metadata": {},
   "source": [
    "Saving the majority vote frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba433e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vals = np.unique(most_sel_list_and_t)\n",
    "count_t_m = np.zeros((len(temperature_list), len(model_list), len(unique_vals)), dtype=int)\n",
    "list_df = []\n",
    "for id_t, t in enumerate(temperature_list):\n",
    "    for id_m, m in enumerate(model_list):\n",
    "        for id_v, v_ in enumerate(unique_vals):\n",
    "            count_t_m[id_t, id_m, id_v] = np.sum(most_sel_list_and_t[id_t, id_m] == v_)\n",
    "    majority_vote_ = pd.DataFrame(index=model_list, columns=unique_vals, data=count_t_m[id_t])\n",
    "    majority_vote_.to_latex(f\"tables/majority_vote_T={t}.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_vote_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aee7a4",
   "metadata": {},
   "source": [
    "Below, dataframes one per temperature, containing number of correct responses across runs (row) and models (column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf299b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a list of dataframes, one per temperature\n",
    "df_accuracy_runs_list = [pd.DataFrame(bm_models_, index=model_list, columns=np.arange(1,11)).T \n",
    "                         for bm_models_ in bm_across_models_and_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d56772",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy_runs_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ecee7",
   "metadata": {},
   "source": [
    "Report hallucination for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_outputs = []\n",
    "index = []\n",
    "\n",
    "for t_ in temperature_list:\n",
    "    for m_ in model_file: \n",
    "        filepath = f\"{path_repetitions}/T={t_}/{m_}/{m_}_promptID_001_repetitions.xlsx\"\n",
    "        dct_counter = evaluate_response_distrib(filepath)\n",
    "        \n",
    "        index.append(f\"T={t_} m={m_}\")\n",
    "        counting_outputs.append([dct_counter[k] for k in sorted(dct_counter.keys())])\n",
    "columns_name = sorted(dct_counter.keys())\n",
    "\n",
    "df_counter = pd.DataFrame(data=counting_outputs, \n",
    "                          index=index,\n",
    "                          columns=columns_name)\n",
    "df_counter.to_latex(\"tables/readability_experiment2.tex\")\n",
    "df_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb519572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_singular_responses(matrix_comparison, \n",
    "                            across_models=True, \n",
    "                            temp_id=1, \n",
    "                            model_id=None,\n",
    "                            saveplot=False,\n",
    "                            plotname=None):\n",
    "    # temp 0 - 0.3, temp 1 - 0.6, temp 2 - 1.\n",
    "    colors_palette = [\n",
    "    \"#b2182b\",  # 0 - deep red\n",
    "    \"#d6604d\",  # 1 - red-orange\n",
    "    \"#f4a582\",  # 2 - orange\n",
    "    \"#fddbc7\",  # 3 - light orange\n",
    "    \"#fee8c8\",  # 4 - beige\n",
    "    \"#f7f7f7\",  # 5 - white/neutral\n",
    "    \"#d1e5f0\",  # 6 - very light blue\n",
    "    \"#92c5de\",  # 7 - light blue\n",
    "    \"#4393c3\",  # 8 - medium blue\n",
    "    \"#2166ac\",  # 9 - dark blue\n",
    "    \"#053061\",  # 10 - deepest blue\n",
    "    ]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(30, 7))\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "    ax.spines['top'].set_linewidth(2)\n",
    "    ax.spines['right'].set_linewidth(2)\n",
    "    ax.tick_params(width=2)\n",
    "    \n",
    "    if across_models: \n",
    "        sorted_id_answer = np.argsort(comparison_mat[temp_id][0]) # temperature 0.6, sorted by Huatuo\n",
    "        matrix_to_iter = comparison_mat[temp_id]\n",
    "        ylabel = model_list\n",
    "        plt.title(f\"LLMs at T={temperature_list[temp_id]}\", fontsize=40, y=1.4)\n",
    "        \n",
    "    else: \n",
    "        sorted_id_answer = np.argsort(comparison_mat[0, model_id]) # temperature 0.6, sorted by Huatuo\n",
    "        matrix_to_iter = comparison_mat[:, model_id]\n",
    "        ylabel = temperature_list\n",
    "        ax.set_ylabel(\"Temperature\", fontsize=25)\n",
    "        plt.title(f\"{model_list[model_id]} at Different Temperatures\", fontsize=40, y=1.4)\n",
    "\n",
    "    n_bars = matrix_to_iter.shape[0]\n",
    "\n",
    "    for i_iterator, votes_model in enumerate(matrix_to_iter):\n",
    "        votes_model = votes_model[sorted_id_answer]\n",
    "        for j, val in enumerate(votes_model):\n",
    "            ax.barh(i_iterator, width=1, left=j, color=colors_palette[val], edgecolor='black')\n",
    "            \n",
    "    sorted_labs = meta_labs_tab_df['labs'].values[sorted_id_answer]\n",
    "    for i_counter, l_ in enumerate(sorted_labs):\n",
    "        if l_:\n",
    "            color = 'white'\n",
    "        else:\n",
    "            color = 'lightgray'\n",
    "        ax.barh(matrix_to_iter.shape[0], width=1, left=i_counter, color=color, edgecolor='black')\n",
    "\n",
    "    # Set y-axis labels\n",
    "    ax.set_yticks(range(n_bars + 1))\n",
    "    ax.set_yticklabels(ylabel + [\"Lab Tests\"], rotation=0, fontsize=12)\n",
    "\n",
    "    # Add legend for shades\n",
    "    from matplotlib.patches import Patch\n",
    "    \n",
    "    text_legend = list(np.arange(11).astype(str)) + ['Yes', 'No']\n",
    "    colors_palette_all = colors_palette + ['white', 'lightgray']\n",
    "    legend_labels = [Patch(facecolor=colors_palette_all[i], label=f'{val}', edgecolor='black') \n",
    "                     for i, val in enumerate(text_legend)]\n",
    "    \n",
    "    ax.legend(handles=legend_labels, title='# Times Correct Option is Selected', \n",
    "              bbox_to_anchor=(0.5, 1.4), loc='upper center',\n",
    "              ncol=13, fontsize=25, title_fontsize=25\n",
    "              )\n",
    "\n",
    "    ax.set_xticks(np.arange(len(sorted_id_answer))+0.5, \n",
    "                  indexes_questions[sorted_id_answer]);\n",
    "\n",
    "    \n",
    "    tick = ax.yaxis.get_major_ticks()\n",
    "    for t in tick:\n",
    "        t.label1.set_fontsize(25) \n",
    "\n",
    "    tick = ax.xaxis.get_major_ticks()\n",
    "    for t in tick:\n",
    "        t.label1.set_fontsize(13) \n",
    "\n",
    "    plt.xlabel(\"Case ID\", fontsize=30)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if saveplot:\n",
    "        plt.savefig(plotname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61672bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9928a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_t, t_ in enumerate(temperature_list):\n",
    "    plotname = f\"plots/responses_exp2_extensive_T={t_}.pdf\"\n",
    "\n",
    "    plot_singular_responses(comparison_mat, \n",
    "                            across_models=True, \n",
    "                            temp_id=id_t, \n",
    "                            model_id=None,\n",
    "                            saveplot=True,\n",
    "                            plotname=plotname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_m, m_ in enumerate(model_list):\n",
    "    plotname = f\"plots/responses_exp2_extensive_m={m_}.pdf\"\n",
    "\n",
    "    plot_singular_responses(comparison_mat, across_models=False, model_id=id_m,\n",
    "                            saveplot=True, plotname=plotname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7236333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_singular_responses(comparison_mat, across_models=True, temp_id=2, model_id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_singular_responses(comparison_mat, across_models=True, temp_id=1, model_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbef376",
   "metadata": {},
   "outputs": [],
   "source": [
    "(comparison_mat[1][0] > comparison_mat[0][0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e612197",
   "metadata": {},
   "outputs": [],
   "source": [
    "(comparison_mat[2][0] > comparison_mat[0][0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce88de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(comparison_mat[2][0] > comparison_mat[1][0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12008ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(comparison_mat[0][0] > comparison_mat[2][0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70576728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: huatuo-o1 correctness over others\n",
    "t_ = 2\n",
    "\n",
    "np.logical_and(np.logical_and(comparison_mat[t_][0] >= comparison_mat[t_][1], \n",
    "                              comparison_mat[t_][0] >= comparison_mat[t_][3]),\n",
    "              comparison_mat[t_][0] >= comparison_mat[t_][2]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c3fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: diabetica-o1 correctness over others\n",
    "\n",
    "np.logical_and(np.logical_and(comparison_mat[t_][1] >= comparison_mat[t_][0],\n",
    "                              comparison_mat[t_][1] >= comparison_mat[t_][3]),\n",
    "               comparison_mat[t_][1] >= comparison_mat[t_][2]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d3c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e814cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# times meditron3 8B superior to huatuo-o1 and diabetica-o1\n",
    "\n",
    "np.logical_and(comparison_mat[1][3] >= comparison_mat[1][0],\n",
    "               comparison_mat[1][3] >= comparison_mat[1][1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f2d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: meditron3-8B correctness over others\n",
    "\n",
    "np.logical_and(np.logical_and(comparison_mat[1][3] >= comparison_mat[1][0],\n",
    "                              comparison_mat[1][3] >= comparison_mat[1][1]),\n",
    "               comparison_mat[1][3] >= comparison_mat[1][2]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: diabetica-7B correctness over others\n",
    "\n",
    "np.logical_and(np.logical_and(comparison_mat[1][2] >= comparison_mat[1][0],\n",
    "                              comparison_mat[1][2] >= comparison_mat[1][1]),\n",
    "               comparison_mat[1][2] >= comparison_mat[1][3]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba4bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huatuo-o1 equivalent or better\n",
    "\n",
    "(np.argmax(comparison_mat, axis=0)==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc596d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals= pd.DataFrame(comparison_mat[0], index=model_list,\n",
    "                   columns=df_all['truth'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00333eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d937380",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa3492",
   "metadata": {},
   "outputs": [],
   "source": [
    "(vals==0).sum(axis=1) # how many times the model never gets the right answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8042ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full range of expected values\n",
    "possible_values = list(range(0, 11))\n",
    "\n",
    "# Count values row-wise\n",
    "counts_per_row = vals.apply(lambda row: row.value_counts().reindex(possible_values, fill_value=0), axis=1)\n",
    "\n",
    "counts_per_row = counts_per_row.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_per_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b033d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_per_row.to_latex(\"T=1_table_repetitions_summary.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9dcc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_per_col = counts_per_row.T\n",
    "\n",
    "counts_per_col\n",
    "\n",
    "counts_per_col.to_latex(\"tables/NEW_table_repetitions_summary.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d89014",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_per_col.sum(axis=1) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a1fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_t, t_ in enumerate(temperature_list):\n",
    "    result = -np.ones((len(model_file), 2), dtype=int)\n",
    "    for id_m, m_ in enumerate(model_file): \n",
    "        idx_correct_10 = np.where(comparison_mat[id_t][id_m] == 10)[0]\n",
    "        n_correct_10 = idx_correct_10.size\n",
    "        n_consist_10 = np.where(count_sel_list_and_t[id_t][id_m] == 10)[0].size \n",
    "        result[id_m, :] = np.array([n_correct_10, n_consist_10 - n_correct_10])\n",
    "    correct_consist = pd.DataFrame(index=model_list, \n",
    "                                   columns=[\"# correct 10 times\", \"# consistently incorrect 10 times\"], \n",
    "                                   data=result)\n",
    "    correct_consist.to_latex(f\"tables/correct_consistent_T={t_}.tex\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6b3b0",
   "metadata": {},
   "source": [
    "# Heatmaps and representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818dc4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10), nrows=3, ncols=4, sharex=True, sharey=True)\n",
    "row_colors = ['#bbbbbb', '#777777', '#000000']  # or use more steps if needed\n",
    "row_colors = [\n",
    "    (0.3060, 0.4285, 0.7891),\n",
    "    'lightsteelblue',\n",
    "    'gold'\n",
    "]\n",
    "\n",
    "global_max = 0\n",
    "for i_t, t in enumerate(temperature_list):\n",
    "    for i_m in range(len(model_list)):\n",
    "        data_heatmap = np.zeros((8, 11))\n",
    "        bm_majority = ~np.isnan(count_sel_list_and_t[i_t][i_m])\n",
    "        for consistency, correctness in zip(count_sel_list_and_t[i_t][i_m][bm_majority],\n",
    "                                            comparison_mat[i_t][i_m][bm_majority]):\n",
    "            data_heatmap[int(consistency)-3, int(correctness)] += 1\n",
    "        data_heatmap = np.vstack((data_heatmap[:5,:].sum(axis=0).reshape(1,-1), \n",
    "                                  data_heatmap[5:,:]))\n",
    "        tmp_max = np.max(data_heatmap) # check, I think it is wrong\n",
    "        # print(tmp_max)\n",
    "        if tmp_max > global_max:\n",
    "            global_max = tmp_max\n",
    "\n",
    "                \n",
    "for i_t, t in enumerate(temperature_list):\n",
    "    df_heatmap_list = []\n",
    "    \n",
    "    for i_m in range(len(model_list)):\n",
    "\n",
    "        data_heatmap = np.zeros((8, 11))\n",
    "        bm_majority = ~np.isnan(count_sel_list_and_t[i_t][i_m]) # there is a majority\n",
    "\n",
    "        for consistency, correctness in zip(count_sel_list_and_t[i_t][i_m][bm_majority], # count of majority vote\n",
    "                                            comparison_mat[i_t][i_m][bm_majority]): # count of correct class\n",
    "\n",
    "            data_heatmap[int(consistency)-3, int(correctness)] += 1\n",
    "            \n",
    "        data_heatmap = np.vstack((data_heatmap[:5,:].sum(axis=0).reshape(1,-1), \n",
    "                                  data_heatmap[5:,:]))\n",
    "        data_heatmap = data_heatmap[::-1]\n",
    "        data = pd.DataFrame(data_heatmap, columns=range(11),\n",
    "                            index=[10, 9, 8, '7 or \\nless'])\n",
    "\n",
    "        ax[i_t, i_m].spines['bottom'].set_linewidth(4)\n",
    "        ax[i_t, i_m].spines['left'].set_linewidth(4)\n",
    "        ax[i_t, i_m].spines['top'].set_linewidth(4)\n",
    "        ax[i_t, i_m].spines['right'].set_linewidth(4)\n",
    "        ax[i_t, i_m].tick_params(width=2)\n",
    "\n",
    "        for spine in ax[i_t, i_m].spines.values():\n",
    "            spine.set_color(row_colors[i_t])\n",
    "            \n",
    "        x_labels = data.columns\n",
    "        y_labels = data.index\n",
    "        x_pos, y_pos, sizes, texts = [], [], [], []\n",
    "        ax[i_t, i_m].grid(True, color='lightgray')\n",
    "        \n",
    "        for i, y in enumerate(y_labels):\n",
    "            for j, x in enumerate(x_labels):\n",
    "                val = int(data.iloc[i, j])\n",
    "                if val > 0:\n",
    "                    x_pos.append(j)\n",
    "                    y_pos.append(i)\n",
    "                    sizes.append(val * 100)  # adjust scale\n",
    "                    texts.append(str(val))\n",
    "\n",
    "        scatter = ax[i_t, i_m].scatter(\n",
    "            x_pos, y_pos,\n",
    "            s=sizes, \n",
    "            c=[int(t) for t in texts],  # values to color by\n",
    "            cmap=\"RdPu\", \n",
    "            edgecolors='none', \n",
    "            marker='o', \n",
    "            zorder=4,\n",
    "            vmin=0, \n",
    "            vmax=global_max,\n",
    "            # cmap='viridis'  # filled blobs\n",
    "        )\n",
    "\n",
    "        # Add number labels to each blob\n",
    "        # for (x, y, t) in zip(x_pos, y_pos, texts):\n",
    "        #    ax[i_t, i_m].text(x, y, t, color='black', ha='center', va='center', fontsize=12, zorder=5)\n",
    "\n",
    "        # === Ticks and labels ===\n",
    "        ax[i_t, i_m].margins(x=0.2, y=0.35)  # add 10% padding on both axes\n",
    "        ax[i_t, i_m].set_xticks(range(len(x_labels)))\n",
    "        ax[i_t, i_m].set_xticklabels(x_labels, fontsize=18)\n",
    "        ax[i_t, i_m].set_yticks(range(len(y_labels)))\n",
    "        ax[i_t, i_m].set_yticklabels(y_labels, fontsize=18)\n",
    "        ax[0, i_m].set_title(model_list[i_m], fontsize=23, weight='bold')\n",
    "       \n",
    "    ax[i_t, 0].set_ylabel(f\"T={t}\", fontsize=20, weight='bold', color=row_colors[i_t], \n",
    "                          # rotation='horizontal'\n",
    "                         )\n",
    "    fig.supxlabel('Frequency of correct option selection\\n     $\\\\bf{Correctness}$', fontsize=25)\n",
    "    fig.supylabel('                  $\\\\bf{Consistency}$\\nFrequency of majority vote selection', fontsize=25)\n",
    "\n",
    "ax[0, 0].invert_yaxis()  # match heatmap orientation\n",
    "\n",
    "sm = cm.ScalarMappable(cmap='RdPu', norm=plt.Normalize(vmin=0, vmax=global_max))\n",
    "sm.set_array([])  # required dummy array\n",
    "\n",
    "# Add colorbar to the figure\n",
    "cbar = fig.colorbar(sm, ax=ax.ravel().tolist(), orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('# questions', fontsize=20)\n",
    "for spine in cbar.ax.spines.values():\n",
    "    spine.set_visible(True)\n",
    "cbar.ax.tick_params(labelsize=18, width=2)\n",
    "cbar.ax.spines['bottom'].set_linewidth(2)\n",
    "cbar.ax.spines['left'].set_linewidth(2)\n",
    "cbar.ax.spines['top'].set_linewidth(2)\n",
    "cbar.ax.spines['right'].set_linewidth(2)\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.savefig(\"plots/correctness_consistency.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca117d2",
   "metadata": {},
   "source": [
    "# Answers Correctness Across Runs at Different Temperatures\n",
    "\n",
    "Number of correct responses across each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a1155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce527952",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,3), ncols=len(model_list), sharey=True)\n",
    "\n",
    "boxprops = dict(linewidth=3)\n",
    "medianprops = dict(linewidth=3)\n",
    "whiskerprops = dict(linewidth=3)\n",
    "capprops = dict(linewidth=3)\n",
    "flierprops = dict(marker='o', markersize=5, linestyle='none')\n",
    "\n",
    "color_t0 = 'black'\n",
    "colors_list = [\n",
    "    (0.3060, 0.4285, 0.7891),\n",
    "    'lightsteelblue',\n",
    "    'gold'\n",
    "]\n",
    "\n",
    "\n",
    "for j, t in enumerate(temperature_list): \n",
    "    # each temperature has a different color\n",
    "    box_color = colors_list[j]\n",
    "\n",
    "    for i, (m, m_name) in enumerate(zip(model_file, model_list)):\n",
    "        ax[i].spines['bottom'].set_linewidth(2)\n",
    "        ax[i].spines['left'].set_linewidth(2)\n",
    "        ax[i].spines['top'].set_linewidth(2)\n",
    "        ax[i].spines['right'].set_linewidth(2)\n",
    "        ax[i].tick_params(width=2)\n",
    "        ax[i].grid()\n",
    "\n",
    "        lab = \"T=0\" if (i == 3 and j == 2) else None\n",
    "        ax[i].axhline(results.loc[f\"{m}-prompt1\"]['# correct'], color=color_t0,\n",
    "                          linewidth=2, linestyle='-', label=lab, zorder=2)\n",
    "\n",
    "        flierprops = dict(marker='o', markersize=5, linestyle='none', \n",
    "                          markerfacecolor=box_color, markeredgecolor=box_color)\n",
    "\n",
    "        bp = ax[i].boxplot(bm_across_models_and_t[j][i], \n",
    "                           positions=[j],\n",
    "                           boxprops=boxprops, \n",
    "                           medianprops=medianprops,\n",
    "                           whiskerprops=whiskerprops, \n",
    "                           capprops=capprops, \n",
    "                           flierprops=flierprops,\n",
    "                           patch_artist=True, \n",
    "                           labels=None,\n",
    "                           # legend=lab,\n",
    "                           zorder=4)\n",
    "                \n",
    "        bp['boxes'][0].set_facecolor(box_color)\n",
    "        for element in ['boxes', 'whiskers', 'caps']: #, 'medians']:\n",
    "            for b in bp[element]:\n",
    "                b.set_color(box_color)\n",
    "                b.set_linewidth(3)\n",
    "        \n",
    "        if t == temperature_list[-1]:\n",
    "            ax[i].set_title(m_name, fontsize=15, weight='bold')\n",
    "        ax[i].set_xticks([])          # removes tick locations\n",
    "        ax[i].set_xticklabels([])     # removes labels (just in case)\n",
    "\n",
    "\n",
    "tick = ax[0].yaxis.get_major_ticks()\n",
    "for t in tick:\n",
    "    t.label.set_fontsize(11) \n",
    "ax[0].set_ylabel(\"# correct responses\", fontsize=16)\n",
    "\n",
    "# legend\n",
    "line_T0 = mlines.Line2D([], [], color='black', label='T=0')\n",
    "# box_handles = [\n",
    "#    mpatches.Patch(facecolor=color, edgecolor=color, label=f'T={temperature_list[k]}')\n",
    "#    for k, color in enumerate(colors_list)]\n",
    "# handles = line_T0 # [line_T0]  + box_handles\n",
    "# ax[-1].legend(handles=handles, fontsize='large') #, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax[-1].legend(fontsize='large')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"plots/temperatures_distribution_runs.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fab658",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_ in df_accuracy_runs_list:\n",
    "    print(df_.describe())\n",
    "    standard_error = df_.sem()\n",
    "    # Calculate the confidence interval\n",
    "    confidence_interval = st.t.interval(0.95, 91 - 1, loc=df_.mean(), scale=standard_error)\n",
    "    print(\"Confidence Intervals:\")\n",
    "    print(confidence_interval)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5677cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5373c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d67f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b531075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced14030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
